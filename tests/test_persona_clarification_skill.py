"""Integration test for persona-clarification skill.

This test verifies:
1. Skill discovery - the skill is found and loaded correctly
2. Skill usage - the agent picks up and uses the skill
3. Outcome validation - the agent produces a persona in the expected format
"""

import re
import shutil
import time
from pathlib import Path

import pytest
from langchain_core.messages import HumanMessage

from deepagents import create_deep_agent
from deepagents.backends.filesystem import FilesystemBackend
from deepagents.middleware.language import LanguageDetectionMiddleware
from deepagents_cli.skills.load import list_skills
from deepagents_cli.skills.middleware import SkillsMiddleware

from tests.model_provider import create_test_model, load_test_model_config
from tests.timing_middleware import TimingMiddleware


@pytest.mark.timeout(180)  # 3 minutes for real LLM calls
def test_persona_clarification_skill_discovery(tmp_path: Path) -> None:
    """Test 1: Verify that persona-clarification skill is discovered correctly.
    
    This test validates:
    - Skill is found in the skills directory
    - YAML frontmatter is parsed correctly
    - Metadata (name, description, path) is extracted
    - SKILL.md file exists and is readable
    """
    # Get the example skill directory
    repo_root = Path(__file__).parent.parent
    example_skill_dir = repo_root / "libs" / "deepagents-cli" / "examples" / "skills" / "persona-clarification"
    
    if not example_skill_dir.exists():
        pytest.skip(f"Example skill directory not found: {example_skill_dir}")
    
    # Copy skill to temporary skills directory
    skills_dir = tmp_path / "skills"
    skills_dir.mkdir(parents=True, exist_ok=True)
    skill_dest = skills_dir / "persona-clarification"
    shutil.copytree(example_skill_dir, skill_dest)
    
    # Test skill discovery using list_skills()
    skills = list_skills(user_skills_dir=skills_dir, project_skills_dir=None)
    
    # Validation 1: Skill should be discovered
    assert len(skills) == 1, f"Should discover exactly one skill, found {len(skills)}"
    
    skill_metadata = skills[0]
    
    # Validation 2: Skill name should match
    assert skill_metadata["name"] == "persona-clarification", f"Skill name should be 'persona-clarification', got '{skill_metadata['name']}'"
    
    # Validation 3: Description should mention persona
    assert "persona" in skill_metadata["description"].lower(), "Description should mention 'persona'"
    
    # Validation 4: Source should be 'user'
    assert skill_metadata["source"] == "user", f"Skill source should be 'user', got '{skill_metadata['source']}'"
    
    # Validation 5: SKILL.md should exist and be readable
    skill_md_path = Path(skill_metadata["path"])
    assert skill_md_path.exists(), f"SKILL.md should exist at {skill_md_path}"
    assert skill_md_path.name == "SKILL.md", f"Path should point to SKILL.md, got {skill_md_path.name}"
    
    # Validation 6: SKILL.md should have proper content
    skill_content = skill_md_path.read_text(encoding="utf-8")
    assert "persona-clarification" in skill_content, "SKILL.md should mention skill name"
    assert "persona" in skill_content.lower(), "SKILL.md should contain persona-related content"
    assert "Name:" in skill_content or "name:" in skill_content, "SKILL.md should have persona structure"
    
    print("\n" + "="*80)
    print("âœ… SKILL DISCOVERY TEST PASSED")
    print("="*80)
    print(f"  Skill Name: {skill_metadata['name']}")
    print(f"  Description: {skill_metadata['description']}")
    print(f"  Path: {skill_metadata['path']}")
    print(f"  Source: {skill_metadata['source']}")


@pytest.mark.timeout(180)  # 3 minutes for real LLM calls
def test_persona_clarification_skill_usage_and_outcome(tmp_path: Path) -> None:
    """Test 2 & 3: Verify skill is used and produces expected outcome.
    
    This test validates:
    - Skill is loaded by SkillsMiddleware
    - Agent picks up the skill when relevant
    - Agent reads the SKILL.md file
    - Agent produces output in the expected persona format
    """
    repo_root = Path(__file__).parent.parent
    cfg = load_test_model_config(repo_root=repo_root)
    try:
        # Set up skills directory
        agent_id = "test_persona_clarification"
        skills_dir = tmp_path / "skills"
        skills_dir.mkdir(parents=True, exist_ok=True)
        
        # Copy persona-clarification skill from examples
        example_skill_dir = repo_root / "libs" / "deepagents-cli" / "examples" / "skills" / "persona-clarification"
        if not example_skill_dir.exists():
            pytest.skip(f"Example skill directory not found: {example_skill_dir}")
        
        skill_dest = skills_dir / "persona-clarification"
        shutil.copytree(example_skill_dir, skill_dest)
        
        # Verify skill discovery
        skills = list_skills(user_skills_dir=skills_dir, project_skills_dir=None)
        assert len(skills) == 1, "Should discover persona-clarification skill"
        assert skills[0]["name"] == "persona-clarification", "Skill name should match"
        
        print("\n" + "="*80)
        print("TEST: PERSONA CLARIFICATION SKILL USAGE AND OUTCOME")
        print("="*80)
        
        # Create model (DeepSeek default; switch to Qwen via BC_API_PROVIDER=qwen + QWEN_* env vars)
        model = create_test_model(cfg=cfg)
        
        # Create agent with SkillsMiddleware
        filesystem_backend = FilesystemBackend(root_dir=str(tmp_path))
        timing_middleware = TimingMiddleware(verbose=True)
        
        agent = create_deep_agent(
            model=model,
            backend=filesystem_backend,
            middleware=[
                timing_middleware,
                SkillsMiddleware(
                    skills_dir=skills_dir,
                    assistant_id=agent_id,
                    project_skills_dir=None,
                ),
            ],
            tools=[],
            system_prompt="""You are a business co-founder assistant helping entrepreneurs develop their startup ideas.

When a user provides a rough or vague business idea, you should use the persona-clarification skill to help them define a clear target user persona.

Follow these steps:
1. Recognize when the user's idea needs persona clarification
2. Read the persona-clarification skill's SKILL.md file
3. Apply the skill's methodology to create a detailed persona
4. Output the persona in the exact format specified in the skill

Be thorough and ask clarifying questions when information is missing.""",
        )
        
        # User request with vague business idea (needs persona clarification)
        user_request = """I have an idea for an app that helps people organize their daily tasks and stay productive. 
        
Can you help me clarify who my target users should be?"""
        
        print(f"\nğŸ“ User Request:\n{user_request}\n")
        print("â³ Starting agent execution...\n")
        
        # Execute the agent
        input_state = {"messages": [HumanMessage(content=user_request)]}
        config = {"configurable": {"thread_id": "test-persona-thread"}}
        
        invoke_start = time.time()
        result = None
        
        try:
            result = agent.invoke(input_state, config)
            invoke_end = time.time()
            invoke_duration = invoke_end - invoke_start
            timing_middleware.total_time = invoke_duration
            
            print(f"\nâœ… Agent execution completed")
            print(f"â±ï¸  Total execution time: {invoke_duration:.2f}s\n")
        except Exception as e:
            invoke_end = time.time()
            invoke_duration = invoke_end - invoke_start
            timing_middleware.total_time = invoke_duration
            print(f"\nâŒ Agent execution failed: {type(e).__name__}: {str(e)[:200]}\n")
            raise
        
        # Extract messages
        messages = result.get("messages", [])
        print(f"ğŸ“Š Messages: {len(messages)} total\n")
        
        # Print timing summary
        timing_middleware.print_summary()
        
        # Validation 2: Skill should be picked up and used
        
        # Check 2a: Agent should read the SKILL.md file
        skill_read = False
        skill_md_path_str = str(skill_dest / "SKILL.md")
        
        for message in messages:
            if message.type == "tool":
                # Check if read_file was called on SKILL.md
                if hasattr(message, 'name') and message.name == "read_file":
                    content = str(message.content)
                    if "persona-clarification" in content.lower() or "persona" in content.lower():
                        skill_read = True
                        print("âœ… Agent read the persona-clarification SKILL.md file")
                        break
                # Also check message content for references to skill
                content = str(message.content)
                if "persona-clarification" in content.lower() or skill_md_path_str in content:
                    skill_read = True
                    print("âœ… Agent accessed persona-clarification skill")
                    break
        
        # Check AI messages for tool calls that read the skill
        for message in messages:
            if message.type == "ai" and hasattr(message, 'tool_calls') and message.tool_calls:
                for tool_call in message.tool_calls:
                    if tool_call.get('name') == 'read_file':
                        args = tool_call.get('args', {})
                        file_path = args.get('file_path', '')
                        if 'persona-clarification' in str(file_path).lower() or 'SKILL.md' in str(file_path):
                            skill_read = True
                            print("âœ… Agent called read_file on persona-clarification SKILL.md")
                            break
                if skill_read:
                    break
        
        # Check final AI response for skill references
        ai_messages = [m for m in messages if m.type == "ai"]
        if ai_messages:
            final_content = str(ai_messages[-1].content).lower()
            if "persona-clarification" in final_content or "persona" in final_content:
                skill_read = True
                print("âœ… Agent response references persona-clarification skill")
        
        # Validation 3: Outcome - Persona format validation
        
        # Extract persona from final AI message
        persona_found = False
        persona_content = None
        
        if ai_messages:
            final_content = str(ai_messages[-1].content)
            persona_content = final_content
            
            # Check for persona structure markers
            persona_markers = [
                "Persona:",
                "Name:",
                "Age:",
                "Background:",
                "Occupation:",
                "Goals:",
                "Core pain points:",
                "pain points",
                "Behaviors:",
                "Environment",
            ]
            
            markers_found = sum(1 for marker in persona_markers if marker.lower() in final_content.lower())
            
            if markers_found >= 3:  # At least 3 persona fields should be present
                persona_found = True
                print(f"âœ… Persona structure detected ({markers_found} fields found)")
        
        # Detailed persona format validation
        print("\n" + "="*80)
        print("OUTCOME VALIDATION")
        print("="*80)
        
        # Required fields based on SKILL.md Output Format section
        # All fields listed in the Output Format are expected, though Name and Income can be optional
        required_persona_fields = [
            ("Name", ["name:", "name -"]),  # Optional but in output format
            ("Age", ["age:", "age range:"]),
            ("Background", ["background:"]),
            ("Occupation", ["occupation:", "role:"]),
            ("Income", ["income", "income range:"]),  # Optional if irrelevant, but in output format
            ("Location", ["location:", "location"]),
            ("Goals", ["goals:", "motivations:", "goal"]),
            ("Pain Points", ["pain point", "pain points", "core pain point", "problem"]),
            ("Behaviors", ["behaviors:", "habits:", "behavior"]),
            ("Environment", ["environment", "product use", "environment of product use"]),
        ]
        
        # Fields that can be omitted if not relevant (based on SKILL.md notes)
        optional_fields = [
            ("Name", ["name:", "name -"]),  # Marked as optional in description
            ("Income", ["income", "income range:"]),  # Marked as "optional if irrelevant"
        ]
        
        if persona_content:
            content_lower = persona_content.lower()
            
            print("\nğŸ“‹ Persona Field Validation:")
            required_found = []
            optional_found = []
            
            for field_name, patterns in required_persona_fields:
                found = any(pattern in content_lower for pattern in patterns)
                is_optional = field_name in [opt[0] for opt in optional_fields]
                
                if found:
                    required_found.append(field_name)
                    marker = "âœ“" if is_optional else "âœ…"
                    print(f"  {marker} {field_name}{' (optional)' if is_optional else ''}")
                else:
                    marker = "âš ï¸" if is_optional else "âŒ"
                    print(f"  {marker} {field_name}{' (optional - can be missing)' if is_optional else ' (missing)'}")
            
            # Track optional fields separately for reporting
            for field_name, patterns in optional_fields:
                found = any(pattern in content_lower for pattern in patterns)
                if found:
                    optional_found.append(field_name)
            
            print(f"\nğŸ“Š Summary:")
            print(f"  Required fields found: {len(required_found)}/{len(required_persona_fields)}")
            print(f"  Optional fields found: {len(optional_found)}/{len(optional_fields)}")
            
            # Validation: At least 7 required fields should be present (out of 10)
            # This ensures a substantial persona is generated
            # Note: Name and Income are technically optional, but we expect most fields
            assert len(required_found) >= 7, (
                f"Persona should contain at least 7 required fields. "
                f"Found: {required_found} ({len(required_found)}/10). "
                f"Missing: {set(f[0] for f in required_persona_fields) - set(required_found)}"
            )
            
            print(f"\nâœ… Persona format validation passed (found {len(required_found)} required fields)")
            
            # Check for clarifying questions section (optional)
            if "clarifying question" in content_lower or "question" in content_lower:
                print("âœ… Clarifying questions section detected")
            
            # Display the persona output
            print("\n" + "="*80)
            print("GENERATED PERSONA OUTPUT")
            print("="*80)
            print(persona_content)
            print("="*80)
        
        # Final validations
        print("\n" + "="*80)
        print("FINAL VALIDATIONS")
        print("="*80)
        
        # Validation 1: Discovery (already tested in first test, but verify here too)
        skills_discovered = list_skills(user_skills_dir=skills_dir, project_skills_dir=None)
        assert len(skills_discovered) == 1, "Skill should be discoverable"
        assert skills_discovered[0]["name"] == "persona-clarification", "Skill name should match"
        print("âœ… Test 1 (Discovery): PASSED - Skill is discoverable")
        
        # Validation 2: Usage
        # Note: We can't always guarantee the agent reads the skill file in a single invocation,
        # but we check for evidence of skill usage
        if skill_read or persona_found:
            print("âœ… Test 2 (Usage): PASSED - Skill was picked up and used")
        else:
            # Check if skill metadata was in system prompt (indirect evidence)
            # Since we can't directly check system prompt, we assume if persona was generated,
            # the skill was likely used
            if persona_found:
                print("âœ… Test 2 (Usage): PASSED - Persona was generated (skill likely used)")
            else:
                pytest.fail("Test 2 (Usage): FAILED - No evidence of skill usage")
        
        # Validation 3: Outcome
        assert persona_found, "Persona should be generated in the response"
        assert len(required_found) >= 5, "Persona should contain at least 5 required fields"
        print("âœ… Test 3 (Outcome): PASSED - Persona format is correct")
        
        print("\n" + "="*80)
        print("ALL TESTS PASSED âœ…")
        print("="*80)
        print("\nSummary:")
        print(f"  1. Discovery: âœ… Skill discovered correctly")
        print(f"  2. Usage: âœ… Skill was picked up and used by agent")
        print(f"  3. Outcome: âœ… Persona generated in correct format ({len(required_found)}/10 fields)")
        
    finally:
        pass


@pytest.mark.timeout(180)
def test_persona_clarification_with_vague_idea(tmp_path: Path) -> None:
    """Test with an even vaguer idea to verify clarifying questions are generated.
    
    This test validates:
    - Skill handles very vague/incomplete ideas
    - Agent generates clarifying questions when information is missing
    - Persona structure is still produced (even if incomplete)
    """
    repo_root = Path(__file__).parent.parent
    cfg = load_test_model_config(repo_root=repo_root)
    try:
        # Set up skills directory
        skills_dir = tmp_path / "skills"
        skills_dir.mkdir(parents=True, exist_ok=True)
        
        example_skill_dir = repo_root / "libs" / "deepagents-cli" / "examples" / "skills" / "persona-clarification"
        if not example_skill_dir.exists():
            pytest.skip(f"Example skill directory not found: {example_skill_dir}")
        
        skill_dest = skills_dir / "persona-clarification"
        shutil.copytree(example_skill_dir, skill_dest)
        
        # Create model and agent
        model = create_test_model(cfg=cfg)
        
        filesystem_backend = FilesystemBackend(root_dir=str(tmp_path))
        
        agent = create_deep_agent(
            model=model,
            backend=filesystem_backend,
            middleware=[
                SkillsMiddleware(
                    skills_dir=skills_dir,
                    assistant_id="test-persona-vague",
                    project_skills_dir=None,
                ),
            ],
            tools=[],
            system_prompt="""You are a business co-founder assistant. When users have vague business ideas, use the persona-clarification skill to help them define their target users and identify missing information.""",
        )
        
        # Very vague idea
        user_request = "I want to make something for young people."
        
        print("\n" + "="*80)
        print("TEST: VAGUE IDEA WITH CLARIFYING QUESTIONS")
        print("="*80)
        print(f"\nğŸ“ User Request (very vague):\n{user_request}\n")
        
        # Execute
        result = agent.invoke(
            {"messages": [HumanMessage(content=user_request)]},
            {"configurable": {"thread_id": "test-vague-idea"}},
        )
        
        messages = result.get("messages", [])
        ai_messages = [m for m in messages if m.type == "ai"]
        
        if ai_messages:
            final_content = str(ai_messages[-1].content)
            
            print("\nğŸ“„ Agent Response:")
            print("-" * 80)
            print(final_content)
            print("-" * 80)
            
            # Check for clarifying questions
            question_indicators = [
                "question",
                "?",
                "what",
                "who",
                "where",
                "when",
                "why",
                "how",
                "clarify",
            ]
            
            has_questions = sum(1 for indicator in question_indicators if indicator.lower() in final_content.lower()) >= 3
            
            # Check for persona structure (might be partial)
            has_persona_structure = (
                "name:" in final_content.lower() or
                "age:" in final_content.lower() or
                "background:" in final_content.lower() or
                "occupation:" in final_content.lower()
            )
            
            print("\nâœ… Outcome Validation:")
            if has_questions:
                print("  âœ… Clarifying questions were generated")
            if has_persona_structure:
                print("  âœ… Persona structure was attempted (even if incomplete)")
            
            # At least one should be true
            assert has_questions or has_persona_structure, (
                "Agent should either generate clarifying questions or attempt to create persona structure"
            )
            
            print("\nâœ… Test passed: Skill handles vague ideas correctly")
        
    finally:
        pass


@pytest.mark.timeout(180)
def test_persona_clarification_with_chinese_input(tmp_path: Path) -> None:
    """Test persona clarification skill with Chinese user input.
    
    This test validates:
    - LanguageDetectionMiddleware detects Chinese from user input
    - Agent responds in Chinese (matching user's language)
    - Skill still works correctly with Chinese input
    - Persona is generated in Chinese with proper structure
    """
    repo_root = Path(__file__).parent.parent
    cfg = load_test_model_config(repo_root=repo_root)
    try:
        # Set up skills directory
        agent_id = "test_persona_clarification_chinese"
        skills_dir = tmp_path / "skills"
        skills_dir.mkdir(parents=True, exist_ok=True)
        
        # Copy persona-clarification skill from examples
        example_skill_dir = repo_root / "libs" / "deepagents-cli" / "examples" / "skills" / "persona-clarification"
        if not example_skill_dir.exists():
            pytest.skip(f"Example skill directory not found: {example_skill_dir}")
        
        skill_dest = skills_dir / "persona-clarification"
        shutil.copytree(example_skill_dir, skill_dest)
        
        # Verify skill discovery
        skills = list_skills(user_skills_dir=skills_dir, project_skills_dir=None)
        assert len(skills) == 1, "Should discover persona-clarification skill"
        assert skills[0]["name"] == "persona-clarification", "Skill name should match"
        
        print("\n" + "="*80)
        print("TEST: PERSONA CLARIFICATION WITH CHINESE INPUT")
        print("="*80)
        
        # Create model
        model = create_test_model(cfg=cfg)
        
        # Create agent with SkillsMiddleware and LanguageDetectionMiddleware
        filesystem_backend = FilesystemBackend(root_dir=str(tmp_path))
        timing_middleware = TimingMiddleware(verbose=True)
        
        agent = create_deep_agent(
            model=model,
            backend=filesystem_backend,
            middleware=[
                timing_middleware,
                LanguageDetectionMiddleware(),  # Add language detection
                SkillsMiddleware(
                    skills_dir=skills_dir,
                    assistant_id=agent_id,
                    project_skills_dir=None,
                ),
            ],
            tools=[],
            system_prompt="""You are a business co-founder assistant helping entrepreneurs develop their startup ideas.

When a user provides a rough or vague business idea, you should use the persona-clarification skill to help them define a clear target user persona.

Follow these steps:
1. Recognize when the user's idea needs persona clarification
2. Read the persona-clarification skill's SKILL.md file
3. Apply the skill's methodology to create a detailed persona
4. Output the persona in the exact format specified in the skill

Be thorough and ask clarifying questions when information is missing.""",
        )
        
        # User request in Chinese with a business idea
        user_request = """æˆ‘æœ‰ä¸€ä¸ªæƒ³æ³•ï¼Œæƒ³åšä¸€ä¸ªå¸®åŠ©äººä»¬ç®¡ç†æ—¥å¸¸ä»»åŠ¡å’Œæé«˜å·¥ä½œæ•ˆç‡çš„åº”ç”¨ã€‚
        
ä½ èƒ½å¸®æˆ‘æ˜ç¡®ä¸€ä¸‹æˆ‘çš„ç›®æ ‡ç”¨æˆ·åº”è¯¥æ˜¯è°å—ï¼Ÿ"""
        
        print(f"\nğŸ“ User Request (Chinese):\n{user_request}\n")
        print("â³ Starting agent execution...\n")
        
        # Execute the agent
        input_state = {"messages": [HumanMessage(content=user_request)]}
        config = {"configurable": {"thread_id": "test-persona-chinese-thread"}}
        
        invoke_start = time.time()
        result = None
        
        try:
            result = agent.invoke(input_state, config)
            invoke_end = time.time()
            invoke_duration = invoke_end - invoke_start
            timing_middleware.total_time = invoke_duration
            
            print(f"\nâœ… Agent execution completed")
            print(f"â±ï¸  Total execution time: {invoke_duration:.2f}s\n")
        except Exception as e:
            invoke_end = time.time()
            invoke_duration = invoke_end - invoke_start
            timing_middleware.total_time = invoke_duration
            print(f"\nâŒ Agent execution failed: {type(e).__name__}: {str(e)[:200]}\n")
            raise
        
        # Extract messages
        messages = result.get("messages", [])
        print(f"ğŸ“Š Messages: {len(messages)} total\n")
        
        # Print timing summary
        timing_middleware.print_summary()
        
        # Validation 1: Language detection - check if Chinese was detected
        # State updates from before_agent are merged into the state, but we need to check
        # the actual state object, not just result.get("state")
        state = result.get("state", {})
        detected_language = state.get("detected_language")
        
        # Also check if language was detected in wrap_model_call (which might store it differently)
        # The middleware detects language in wrap_model_call if not in state
        print("\n" + "="*80)
        print("LANGUAGE DETECTION VALIDATION")
        print("="*80)
        
        if detected_language:
            print(f"âœ… Language detected in state: {detected_language}")
            # Chinese language codes: 'zh', 'zh-cn', 'zh-tw'
            assert detected_language.startswith("zh"), (
                f"Expected Chinese language code (zh*), got {detected_language}"
            )
        else:
            # Language might be detected but not stored in state if langdetect isn't available
            # or if detection happens in wrap_model_call. The important thing is that
            # the agent responded in Chinese, which we validate next.
            print("âš ï¸  Language not detected in state (may be detected in middleware)")
            print("   Note: Language detection in wrap_model_call may not persist to state")
        
        # Validation 2: Response language - check if agent responded in Chinese
        ai_messages = [m for m in messages if m.type == "ai"]
        assert len(ai_messages) > 0, "Agent should have generated at least one response"
        
        final_content = str(ai_messages[-1].content)
        
        # Check for Chinese characters (Unicode range for CJK Unified Ideographs)
        chinese_char_pattern = re.compile(r'[\u4e00-\u9fff]+')
        has_chinese = bool(chinese_char_pattern.search(final_content))
        
        print("\n" + "="*80)
        print("RESPONSE LANGUAGE VALIDATION")
        print("="*80)
        
        if has_chinese:
            chinese_chars = chinese_char_pattern.findall(final_content)
            chinese_char_count = sum(len(match) for match in chinese_chars)
            print(f"âœ… Response contains Chinese characters: {chinese_char_count} characters found")
            print(f"   Sample Chinese text: {chinese_chars[0][:50] if chinese_chars else 'N/A'}...")
        else:
            print("âš ï¸  Response does not contain Chinese characters")
            print("   This may indicate the language detection middleware didn't work as expected")
        
        # Validation 3: Persona structure in Chinese response
        print("\n" + "="*80)
        print("PERSONA STRUCTURE VALIDATION (Chinese)")
        print("="*80)
        
        # Chinese keywords for persona fields
        chinese_persona_fields = [
            ("Name", ["å§“å", "åå­—", "åç§°"]),
            ("Age", ["å¹´é¾„", "å²"]),
            ("Background", ["èƒŒæ™¯", "ç»å†"]),
            ("Occupation", ["èŒä¸š", "å·¥ä½œ", "èŒä½", "è§’è‰²"]),
            ("Income", ["æ”¶å…¥", "è–ªèµ„", "å·¥èµ„"]),
            ("Location", ["åœ°ç‚¹", "ä½ç½®", "åœ°åŒº", "åŸå¸‚"]),
            ("Goals", ["ç›®æ ‡", "ç›®çš„", "åŠ¨æœº"]),
            ("Pain Points", ["ç—›ç‚¹", "é—®é¢˜", "å›°éš¾", "æŒ‘æˆ˜"]),
            ("Behaviors", ["è¡Œä¸º", "ä¹ æƒ¯", "æ–¹å¼"]),
            ("Environment", ["ç¯å¢ƒ", "åœºæ™¯", "ä½¿ç”¨ç¯å¢ƒ"]),
        ]
        
        content_lower = final_content.lower()
        found_fields = []
        
        print("\nğŸ“‹ Persona Field Validation (Chinese):")
        for field_name, keywords in chinese_persona_fields:
            # Check for Chinese keywords
            found = any(keyword in final_content for keyword in keywords)
            if found:
                found_fields.append(field_name)
                print(f"  âœ… {field_name} (found)")
            else:
                print(f"  âŒ {field_name} (missing)")
        
        # Also check for English field names (in case agent mixed languages)
        english_persona_fields = [
            ("Name", ["name:", "name -"]),
            ("Age", ["age:", "age range:"]),
            ("Background", ["background:"]),
            ("Occupation", ["occupation:", "role:"]),
            ("Location", ["location:"]),
            ("Goals", ["goals:", "motivations:"]),
            ("Pain Points", ["pain point", "pain points"]),
            ("Behaviors", ["behaviors:", "habits:"]),
            ("Environment", ["environment"]),
        ]
        
        english_found = []
        for field_name, patterns in english_persona_fields:
            found = any(pattern in content_lower for pattern in patterns)
            if found and field_name not in found_fields:
                english_found.append(field_name)
        
        if english_found:
            print(f"\nâš ï¸  Also found English field names: {english_found}")
            print("   (Agent may have mixed languages)")
        
        total_fields_found = len(found_fields) + len(english_found)
        
        print(f"\nğŸ“Š Summary:")
        print(f"  Chinese fields found: {len(found_fields)}/{len(chinese_persona_fields)}")
        print(f"  English fields found: {len(english_found)}")
        print(f"  Total fields found: {total_fields_found}")
        
        # Validation: At least 4 persona fields should be present
        # Note: Chinese responses may have fewer explicit field labels but still contain
        # the information in a more narrative format. We require at least 4 fields.
        assert total_fields_found >= 4, (
            f"Persona should contain at least 4 fields. "
            f"Found: {total_fields_found} (Chinese: {len(found_fields)}, English: {len(english_found)})"
        )
        
        # Validation 4: Display the response
        print("\n" + "="*80)
        print("AGENT RESPONSE (Chinese)")
        print("="*80)
        print(final_content)
        print("="*80)
        
        # Final validations
        print("\n" + "="*80)
        print("FINAL VALIDATIONS")
        print("="*80)
        
        # Language validation
        if has_chinese:
            print("âœ… Language: Agent responded in Chinese")
        else:
            print("âš ï¸  Language: Agent response may not be in Chinese")
        
        # Persona validation
        if total_fields_found >= 4:
            print(f"âœ… Persona: Generated with {total_fields_found} fields")
        else:
            pytest.fail(f"Persona validation failed: only {total_fields_found} fields found")
        
        # Skill usage validation (check if skill was referenced)
        skill_referenced = (
            "persona" in content_lower or
            "ç”¨æˆ·ç”»åƒ" in final_content or
            "ç›®æ ‡ç”¨æˆ·" in final_content or
            "persona-clarification" in content_lower
        )
        
        if skill_referenced:
            print("âœ… Skill: Persona clarification skill was used")
        
        print("\n" + "="*80)
        print("CHINESE INPUT TEST PASSED âœ…")
        print("="*80)
        print("\nSummary:")
        print(f"  1. Language Detection: {'âœ…' if detected_language and detected_language.startswith('zh') else 'âš ï¸'}")
        print(f"  2. Response Language: {'âœ… Chinese' if has_chinese else 'âš ï¸ May not be Chinese'}")
        print(f"  3. Persona Generation: âœ… {total_fields_found} fields found")
        print(f"  4. Skill Usage: {'âœ…' if skill_referenced else 'âš ï¸'}")
        
    finally:
        pass

